import sys
import torch
from transformers import ElectraTokenizer, ElectraForSequenceClassification

# âœ… ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
model_path = "C:/Users/SSAFY/Desktop/lumina/S12P31S306/ai/goodness-electra-v3"
tokenizer = ElectraTokenizer.from_pretrained(model_path)
model = ElectraForSequenceClassification.from_pretrained(model_path)
model.eval()


# âœ… ë¬¸ì¥ ì…ë ¥ ë°›ì•„ ì˜ˆì¸¡
def predict(text):
    inputs = tokenizer(
        text, return_tensors="pt", truncation=True, padding=True, max_length=128
    )
    with torch.no_grad():
        outputs = model(**inputs)
        probs = torch.softmax(outputs.logits, dim=-1)
        pred = torch.argmax(probs, dim=1).item()
    return pred, probs.squeeze().tolist()


# âœ… í…ŒìŠ¤íŠ¸ ë£¨í”„
label_dict = {0: "ì„ í•œ ìƒê° + ì„ í•œ í–‰ë™", 1: "ì„ í•œ ìƒê°ë§Œ", 2: "ì¤‘ë¦½", 3: "ë‚˜ìœ ë§"}

print("ğŸ‘‰ í…ŒìŠ¤íŠ¸í•  ë¬¸ì¥ì„ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œí•˜ë ¤ë©´ q ì…ë ¥):")
while True:
    try:
        # âœ… readlineì„ ì§ì ‘ UTF-8ë¡œ ë””ì½”ë”©
        raw = sys.stdin.buffer.readline()
        if not raw:
            break
        text = raw.decode("utf-8", errors="ignore").strip()
        if text.lower() == "q":
            break
        pred, probs = predict(text)
        print(f"âœ… ì˜ˆì¸¡ ê²°ê³¼: {label_dict[pred]} ({pred}) (í™•ë¥ : {probs[pred]:.2f})\n")
    except Exception as e:
        print(f"âš ï¸ ì˜ˆì™¸ ë°œìƒ: {e}")
